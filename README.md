# Coursework-link
Postgraduate assignment
##### Related code link

paper： [VideoQA in the Era of LLMs: An Empirical Study ](https://arxiv.org/abs/2408.04223)

code：https://github.com/doc-doc/VideoQA-LLMs

paper：[BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/pdf/2301.12597v3.pdf)

code：https://github.com/salesforce/lavis

paper：[Visual Instruction Tuning](https://arxiv.org/pdf/2304.08485v2.pdf)

code：https://github.com/haotian-liu/LLaVA

paper：[Self-Chained Image-Language Model for Video Localization and Question Answering](https://arxiv.org/pdf/2305.06988v2.pdf)

code：https://github.com/yui010206/sevila

paper：[Large Language Models are Temporal and Causal Reasoners for Video Question Answering](https://arxiv.org/pdf/2310.15747v2.pdf)

code：https://github.com/mlvlab/Flipped-VQA

paper：[Can I Trust Your Answer? Visually Grounded Video Question Answering](https://arxiv.org/pdf/2309.01327v2.pdf)

code：https://github.com/doc-doc/next-gqa

paper：[Learning Video Representations from Large Language Models](https://arxiv.org/pdf/2212.04501v1.pdf)

code：https://github.com/facebookresearch/lavila

paper：[Video-LLaVA: Learning United Visual Representation by Alignment Before Projection](https://arxiv.org/pdf/2311.10122v3.pdf)

code：https://github.com/PKU-YuanGroup/Video-LLaVA

paper：[ViperGPT: Visual Inference via Python Execution for Reasoning](https://arxiv.org/pdf/2303.08128v1.pdf)

code：https://github.com/cvlab-columbia/viper

paper：[Hallucination of Multimodal Large Language Models: A Survey](https://arxiv.org/pdf/2404.18930v1.pdf)

code：https://github.com/showlab/awesome-mllm-hallucination

paper：[A Survey on Multimodal Large Language Models](https://arxiv.org/pdf/2306.13549v2.pdf)

code：https://github.com/bradyfu/awesome-multimodal-large-language-models

